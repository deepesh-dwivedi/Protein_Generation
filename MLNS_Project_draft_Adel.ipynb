{"cells":[{"cell_type":"markdown","metadata":{"id":"7hLlrFmLo3Ju"},"source":["## MLNS Project Protein Protein Interaction\n","\n","Currently I have downloaded the Protein and Enzyme data set\n","Following lab 4 of MLNS for Enzymes"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11521,"status":"ok","timestamp":1682165174213,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"Tct_w9nC05yr","outputId":"24e80c9d-f9a1-4854-96b6-2fd07776cb1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dgl\n","  Downloading dgl-1.0.1-cp39-cp39-manylinux1_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.27.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n","Installing collected packages: dgl\n","Successfully installed dgl-1.0.1\n"]}],"source":["! pip install dgl"]},{"cell_type":"code","source":[],"metadata":{"id":"cfob9nNKTe_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UbVLIrFTTfNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5668,"status":"ok","timestamp":1682165268327,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"k9BWOfiIyQul","colab":{"base_uri":"https://localhost:8080/"},"outputId":"26bca003-2637-452c-c4b3-cde3cdfeea92"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}],"source":["# Import packages\n","import dgl\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from dgl.dataloading import GraphDataLoader\n","from dgl.nn import GraphConv\n","from IPython.display import Latex\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3492,"status":"ok","timestamp":1682165271809,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"M2ZgU3AUxbMk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5778c8ec-4bfe-474d-9e67-a5240505e1bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading /root/.dgl/ENZYMES.zip from https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip...\n","Extracting file to /root/.dgl/ENZYMES\n"]}],"source":["dataset_enzymes = dgl.data.TUDataset(name='ENZYMES')\n","\n","# Add self loop to each graph\n","dataset_enzymes.graph_lists = [dgl.add_self_loop(graph) for graph in dataset_enzymes.graph_lists]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682165273843,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"b40ReIpYxbMk","outputId":"95434e7b-b117-457e-e835-a138a421707d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Graph(num_nodes=37, num_edges=205,\n","       ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n"," tensor([5]))"]},"metadata":{},"execution_count":4}],"source":["dataset_enzymes[0]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":843,"status":"ok","timestamp":1682165277638,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"y4GJlYAqxbMk","outputId":"084bfc80-87dd-4fef-ec43-82a317fc9708"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of graph categories: 6\n","Dimension of nodes features 18\n"]}],"source":["print('Number of graph categories:', dataset_enzymes.num_labels)\n","print('Dimension of nodes features', dataset_enzymes[0][0].ndata['node_attr'].shape[1])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"M9lqidPdxbMk","executionInfo":{"status":"ok","timestamp":1682165280570,"user_tz":-120,"elapsed":4,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["# Split dataset into train, validation and test sets\n","train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n","        dataset_enzymes, frac_list=[0.6, 0.2, 0.2], shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TxLx9RS5xbMm","executionInfo":{"status":"ok","timestamp":1682165284796,"user_tz":-120,"elapsed":461,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["# batch graphs with GraphDataLoader\n","train_dataloader = GraphDataLoader(\n","        train_sampler, batch_size=5, drop_last=False)\n","val_dataloader = GraphDataLoader(\n","    val_sampler, batch_size=5, drop_last=False)\n","test_dataloader = GraphDataLoader(\n","    test_sampler, batch_size=5, drop_last=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"IX1ByyHPxbMm","executionInfo":{"status":"ok","timestamp":1682165287469,"user_tz":-120,"elapsed":7,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["'''\n","class BasicGraphModel(torch.nn.Module):\n","\n","    def __init__(self, n_layers, input_size, hidden_size, output_size):\n","        super(BasicGraphModel, self).__init__()\n","\n","        # Define GNN components\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GraphConv(input_size, hidden_size))\n","        for i in range(n_layers-1):\n","            self.convs.append(GraphConv(hidden_size, hidden_size))\n","        self.linear = torch.nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, g, x):\n","        # Message Passing -- Learn node representations via GCN\n","        for conv in self.convs[:-1]:\n","            x = conv(g, x)\n","            x = F.elu(x)\n","        x = self.convs[-1](g, x)\n","        # Readout -- average all node representations to get graph embedding\n","        g.ndata['h'] = x\n","        x = dgl.mean_nodes(g, 'h')\n","        # Apply linear layer to classify graph representation\n","        x = self.linear(x)\n","        return x\n","'''\n","class BasicGraphModel(torch.nn.Module):\n","\n","    def __init__(self, n_layers, input_size, hidden_size, output_size):\n","        super(BasicGraphModel, self).__init__()\n","\n","        # Define GNN components\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GraphConv(input_size, hidden_size))\n","        for i in range(n_layers-1):\n","            self.convs.append(GraphConv(hidden_size, hidden_size))\n","        self.linear = torch.nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, g, x):\n","        # Message Passing -- Learn node representations via GCN\n","        for conv in self.convs[:-1]:\n","            x = conv(g, x)\n","            x = F.leaky_relu(x)\n","        x = self.convs[-1](g, x)\n","        # Readout -- average all node representations to get graph embedding\n","        g.ndata['h'] = x\n","        x = dgl.mean_nodes(g, 'h')\n","        # Apply linear layer to classify graph representation\n","        x = self.linear(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"Zm7_zqrsxbMm"},"source":["### 2.1 Training and evaluation"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"u3BkZRSpxbMm","executionInfo":{"status":"ok","timestamp":1682165292101,"user_tz":-120,"elapsed":507,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["def train(model, loss_fcn, optimizer, train_dataloader, val_dataloader, num_epochs):\n","    model = model.double()\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        losses = []\n","        for batch, batched_graph in enumerate(train_dataloader):\n","            batched_graph, labels = batched_graph\n","            logits = model(batched_graph, batched_graph.ndata['node_attr'].double())\n","            loss = loss_fcn(logits, labels.T[0])\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            losses.append(loss.item())\n","        loss_data = np.mean(losses)\n","\n","        if epoch % 5 == 0:\n","            print(\"Epoch {} | Loss: {:.4f}\".format(epoch, loss_data))\n","            test(model, loss_fcn, val_dataloader)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bEl3rCa3xbMm","executionInfo":{"status":"ok","timestamp":1682165293778,"user_tz":-120,"elapsed":6,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["def test(model, loss_fcn, dataloader):\n","    scores = []\n","    for batch, batched_graph in enumerate(dataloader):\n","        batched_graph, labels = batched_graph\n","        scores.append(\n","            evaluate(model, batched_graph, labels, loss_fcn))\n","    mean_scores = np.mean(scores)\n","    print(\"Accuracy score: {:.4f}\".format(mean_scores))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CeIVMfNexbMm","executionInfo":{"status":"ok","timestamp":1682165296183,"user_tz":-120,"elapsed":16,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"}}},"outputs":[],"source":["def evaluate(model, batched_graph, labels, loss_fcn):\n","    model = model.double()\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(batched_graph, batched_graph.ndata['node_attr'].double())\n","\n","    labels = labels.T[0]\n","    loss = loss_fcn(output, labels)\n","    predict = output.argmax(dim=1)\n","    score = (labels == predict).sum().item() / len(labels)\n","\n","    return score"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84708,"status":"ok","timestamp":1682165382990,"user":{"displayName":"Adel REMADI","userId":"07812175398000876215"},"user_tz":-120},"id":"vBaokYNbxbMn","outputId":"e18e804e-fbfb-4128-a61d-fd578e0adcf4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  assert input.numel() == input.storage().size(), (\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0 | Loss: 2.1919\n","Accuracy score: 0.2250\n","Epoch 5 | Loss: 1.7149\n","Accuracy score: 0.2000\n","Epoch 10 | Loss: 1.6954\n","Accuracy score: 0.2000\n","Epoch 15 | Loss: 1.6791\n","Accuracy score: 0.2083\n","Epoch 20 | Loss: 1.6469\n","Accuracy score: 0.2417\n","Epoch 25 | Loss: 1.5984\n","Accuracy score: 0.2750\n","Epoch 30 | Loss: 1.5538\n","Accuracy score: 0.2667\n","Epoch 35 | Loss: 1.5222\n","Accuracy score: 0.3333\n","Epoch 40 | Loss: 1.4760\n","Accuracy score: 0.3417\n","Epoch 45 | Loss: 1.4204\n","Accuracy score: 0.3583\n","Epoch 50 | Loss: 1.3656\n","Accuracy score: 0.3833\n","Epoch 55 | Loss: 1.3074\n","Accuracy score: 0.3667\n","Epoch 60 | Loss: 1.2531\n","Accuracy score: 0.3583\n","Epoch 65 | Loss: 1.1992\n","Accuracy score: 0.3583\n","Epoch 70 | Loss: 1.1548\n","Accuracy score: 0.3167\n","Epoch 75 | Loss: 1.1191\n","Accuracy score: 0.3167\n","Epoch 80 | Loss: 1.0748\n","Accuracy score: 0.3750\n","Epoch 85 | Loss: 0.9889\n","Accuracy score: 0.3833\n","Epoch 90 | Loss: 0.9679\n","Accuracy score: 0.3833\n","Epoch 95 | Loss: 0.9234\n","Accuracy score: 0.3833\n","Epoch 100 | Loss: 0.8408\n","Accuracy score: 0.4167\n","Epoch 105 | Loss: 0.8358\n","Accuracy score: 0.4250\n","Epoch 110 | Loss: 0.7826\n","Accuracy score: 0.4167\n","Epoch 115 | Loss: 0.8307\n","Accuracy score: 0.4167\n","Epoch 120 | Loss: 0.7376\n","Accuracy score: 0.3833\n","Epoch 125 | Loss: 0.8188\n","Accuracy score: 0.4000\n","Epoch 130 | Loss: 0.6151\n","Accuracy score: 0.4083\n","Epoch 135 | Loss: 0.6061\n","Accuracy score: 0.4250\n","Epoch 140 | Loss: 0.6433\n","Accuracy score: 0.3917\n","Epoch 145 | Loss: 0.5047\n","Accuracy score: 0.3833\n","Accuracy score: 0.4083\n"]}],"source":["# Store features\n","n_features, n_classes = dataset_enzymes[0][0].ndata['node_attr'].shape[1], \\\n","    dataset_enzymes.num_labels\n","hidden_size = 128\n","\n","# Define model, loss function and optimizer\n","model = BasicGraphModel(n_layers=3, input_size=n_features,\n","                        hidden_size=hidden_size, output_size=n_classes)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","loss_fcn = torch.nn.CrossEntropyLoss()\n","\n","# Train and test\n","train(model, loss_fcn, optimizer,\n","        train_dataloader, val_dataloader, num_epochs=150)\n","test(model, loss_fcn, test_dataloader)\n"]},{"cell_type":"markdown","metadata":{"id":"r1sqk-3QEbdH"},"source":["## Doing everything with Proteins"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3765,"status":"ok","timestamp":1681316864493,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"Jp9iSMvoElZn","outputId":"a0b3b225-130e-49e4-bcf2-8fbce23b1089"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading /root/.dgl/PROTEINS.zip from https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip...\n","Extracting file to /root/.dgl/PROTEINS\n"]}],"source":["dataset_proteins = dgl.data.TUDataset(name='PROTEINS')\n","\n","# Add self loop to each graph\n","dataset_proteins.graph_lists = [dgl.add_self_loop(graph) for graph in dataset_proteins.graph_lists]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258,"status":"ok","timestamp":1681316866769,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"JbnZitMKElZn","outputId":"53c66a76-4527-4fa0-e0f9-e377b9a4f276"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Graph(num_nodes=42, num_edges=204,\n","       ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(1,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n"," tensor([0]))"]},"metadata":{},"execution_count":15}],"source":["dataset_proteins[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681316869134,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"PLl_papjElZo","outputId":"d2fcad54-a635-4edd-8a27-e36d54abdac8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of graph categories: 2\n","Dimension of nodes features 1\n"]}],"source":["print('Number of graph categories:', dataset_proteins.num_labels)\n","print('Dimension of nodes features', dataset_proteins[0][0].ndata['node_attr'].shape[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EQedI7NElZo"},"outputs":[],"source":["# Split dataset into train, validation and test sets\n","train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n","        dataset_proteins, frac_list=[0.6, 0.2, 0.2], shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0FwZN4aElZo"},"outputs":[],"source":["# batch graphs with GraphDataLoader\n","train_dataloader = GraphDataLoader(\n","        train_sampler, batch_size=5, drop_last=False)\n","val_dataloader = GraphDataLoader(\n","    val_sampler, batch_size=5, drop_last=False)\n","test_dataloader = GraphDataLoader(\n","    test_sampler, batch_size=5, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298702,"status":"ok","timestamp":1680628818678,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"195A9BYOE43U","outputId":"77660ff7-dd52-4fbb-ef44-5ee4ea900e0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 | Loss: 0.6883\n","Accuracy score: 0.5844\n","Epoch 5 | Loss: 0.6759\n","Accuracy score: 0.5889\n","Epoch 10 | Loss: 0.6718\n","Accuracy score: 0.6156\n","Epoch 15 | Loss: 0.6620\n","Accuracy score: 0.6267\n","Epoch 20 | Loss: 0.6631\n","Accuracy score: 0.6222\n","Epoch 25 | Loss: 0.6539\n","Accuracy score: 0.6311\n","Epoch 30 | Loss: 0.6514\n","Accuracy score: 0.6267\n","Epoch 35 | Loss: 0.6499\n","Accuracy score: 0.6133\n","Epoch 40 | Loss: 0.6490\n","Accuracy score: 0.6133\n","Epoch 45 | Loss: 0.6515\n","Accuracy score: 0.6178\n","Epoch 50 | Loss: 0.6480\n","Accuracy score: 0.6133\n","Epoch 55 | Loss: 0.6475\n","Accuracy score: 0.6133\n","Epoch 60 | Loss: 0.6472\n","Accuracy score: 0.6178\n","Epoch 65 | Loss: 0.6456\n","Accuracy score: 0.6089\n","Epoch 70 | Loss: 0.6450\n","Accuracy score: 0.6133\n","Epoch 75 | Loss: 0.6446\n","Accuracy score: 0.6044\n","Epoch 80 | Loss: 0.6448\n","Accuracy score: 0.6089\n","Epoch 85 | Loss: 0.6447\n","Accuracy score: 0.6089\n","Epoch 90 | Loss: 0.6441\n","Accuracy score: 0.6044\n","Epoch 95 | Loss: 0.6441\n","Accuracy score: 0.6044\n","Epoch 100 | Loss: 0.6444\n","Accuracy score: 0.6089\n","Epoch 105 | Loss: 0.6431\n","Accuracy score: 0.6178\n","Epoch 110 | Loss: 0.6492\n","Accuracy score: 0.6222\n","Epoch 115 | Loss: 0.6431\n","Accuracy score: 0.6222\n","Epoch 120 | Loss: 0.6426\n","Accuracy score: 0.6178\n","Epoch 125 | Loss: 0.6423\n","Accuracy score: 0.6222\n","Epoch 130 | Loss: 0.6419\n","Accuracy score: 0.6222\n","Epoch 135 | Loss: 0.6417\n","Accuracy score: 0.6222\n","Epoch 140 | Loss: 0.6429\n","Accuracy score: 0.6178\n","Epoch 145 | Loss: 0.6407\n","Accuracy score: 0.6178\n","Accuracy score: 0.6767\n"]}],"source":["# Store features\n","n_features, n_classes = dataset_proteins[0][0].ndata['node_attr'].shape[1], \\\n","    dataset_proteins.num_labels\n","hidden_size = 64\n","\n","# Define model, loss function and optimizer\n","model = BasicGraphModel(n_layers=5, input_size=n_features,\n","                        hidden_size=hidden_size, output_size=n_classes)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","loss_fcn = torch.nn.CrossEntropyLoss()\n","\n","# Train and test\n","train(model, loss_fcn, optimizer,\n","        train_dataloader, val_dataloader, num_epochs=150)\n","test(model, loss_fcn, test_dataloader)\n"]},{"cell_type":"markdown","metadata":{"id":"xIt3a-Ts8MIw"},"source":["## Trying state of the art model GCN based on github paper using a pooling layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22034,"status":"ok","timestamp":1680774153842,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"WswXn_4A8daN","outputId":"273c7c98-32a9-47f2-e34f-eb7746605641"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.27.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.0.12)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.1.1)\n","Building wheels for collected packages: torch_geometric\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=a9c8ffed22212cad75d93c32e30e2720a0f4b75a431dccbaa7f9b1ab41c3e920\n","  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n","Successfully built torch_geometric\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.3.0\n"]}],"source":["! pip install torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7782,"status":"ok","timestamp":1680774510351,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"CNLIFDCW8plE","outputId":"d5b9dc7a-4802-4312-93aa-eb60b84d30a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: layers in /usr/local/lib/python3.9/dist-packages (0.1.5)\n","Requirement already satisfied: bashutils in /usr/local/lib/python3.9/dist-packages (from layers) (0.0.4)\n","Requirement already satisfied: PyYaml in /usr/local/lib/python3.9/dist-packages (from layers) (6.0)\n"]}],"source":["! pip install layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":398895,"status":"ok","timestamp":1680775076761,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"fXR1rHj09WxS","outputId":"563fa739-57bd-46c2-e6ac-f21add68d57e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_scatter\n","  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: torch_scatter\n","  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_scatter: filename=torch_scatter-2.1.1-cp39-cp39-linux_x86_64.whl size=492132 sha256=934c89a2f79c2ad245039aba1618d4ea49ce7f14455fcf5b361970816d800799\n","  Stored in directory: /root/.cache/pip/wheels/d5/0c/18/11b4cf31446c5d460543b0fff930fcac3a3f8a785e5c73fb15\n","Successfully built torch_scatter\n","Installing collected packages: torch_scatter\n","Successfully installed torch_scatter-2.1.1\n"]}],"source":["! pip install torch_scatter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"2Q9V8SqKATjv","outputId":"00a5ca7b-4c99-4303-fb15-89f856fa2f94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_sparse\n","  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch_sparse) (1.22.4)\n","Building wheels for collected packages: torch_sparse\n","  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_sparse: filename=torch_sparse-0.6.17-cp39-cp39-linux_x86_64.whl size=1082944 sha256=0db7882b5c3ef1beb7f76cd43c167a7c1e636bfd3baf7874556d94769272d279\n","  Stored in directory: /root/.cache/pip/wheels/f8/43/54/bcb8acdd1109bd1e4c71106747af298d0315cdf3f090b2ae43\n","Successfully built torch_sparse\n","Installing collected packages: torch_sparse\n","Successfully installed torch_sparse-0.6.17\n"]}],"source":["! pip install torch_sparse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":209,"status":"error","timestamp":1680775119926,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"},"user_tz":-120},"id":"gdcE2Uop8K_z","outputId":"effc3481-e4b6-4ee2-ce7c-487bc40480f5"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-513c206810fb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHGPSLPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/layers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_to_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_remaining_self_loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspspmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n","from torch_geometric.nn import GCNConv\n","\n","from layers import GCN, HGPSLPool\n","\n","\n","class Model(torch.nn.Module):\n","    def __init__(self, args):\n","        super(Model, self).__init__()\n","        self.args = args\n","        self.num_features = args.num_features\n","        self.nhid = args.nhid\n","        self.num_classes = args.num_classes\n","        self.pooling_ratio = args.pooling_ratio\n","        self.dropout_ratio = args.dropout_ratio\n","        self.sample = args.sample_neighbor\n","        self.sparse = args.sparse_attention\n","        self.sl = args.structure_learning\n","        self.lamb = args.lamb\n","\n","        self.conv1 = GCNConv(self.num_features, self.nhid)\n","        self.conv2 = GCN(self.nhid, self.nhid)\n","        self.conv3 = GCN(self.nhid, self.nhid)\n","\n","        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n","        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n","\n","        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n","        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n","        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        edge_attr = None\n","\n","        x = F.relu(self.conv1(x, edge_index, edge_attr))\n","        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n","        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","\n","        x = F.relu(self.conv2(x, edge_index, edge_attr))\n","        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n","        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","\n","        x = F.relu(self.conv3(x, edge_index, edge_attr))\n","        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","\n","        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n","\n","        x = F.relu(self.lin1(x))\n","        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n","        x = F.relu(self.lin2(x))\n","        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n","        x = F.log_softmax(self.lin3(x), dim=-1)\n","\n","        return x"]},{"cell_type":"code","source":["!pip install torch-geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSYRjEZLUcr1","executionInfo":{"status":"ok","timestamp":1681317300450,"user_tz":-120,"elapsed":11699,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"}},"outputId":"13f9ab74-90fb-4b45-b65b-e639aa821b22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/616.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/616.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=386da170ff7a0956ae05d56d3f717ea0a928d8b242f1f7b44c80b68f4847b34b\n","  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.3.0\n"]}]},{"cell_type":"code","source":["dataset_proteins[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_L_MWy_WbsI","executionInfo":{"status":"ok","timestamp":1681317815398,"user_tz":-120,"elapsed":1,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"}},"outputId":"bd7f4526-4c10-4113-e484-0eafeda386fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Graph(num_nodes=42, num_edges=204,\n","       ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(1,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n"," tensor([0]))"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Generating Protein Sequences using GAT"],"metadata":{"id":"Aq7U7G17NpUz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vwrcpfy0y9V","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1681317976318,"user_tz":-120,"elapsed":175,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"}},"outputId":"b837eeb9-beaa-4a35-b4d1-8b8a78b02830"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-21a6817bf178>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-21a6817bf178>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"]}],"source":["import torch\n","import torch.nn.functional as F\n","#from torch_geometric.datasets import TUDataset\n","#from torch_geometric.data import DataLoader\n","from torch_geometric.nn import GATConv\n","import random\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, num_node_features, hidden_channels, num_classes):\n","        super(GAT, self).__init__()\n","        self.conv1 = GATConv(num_node_features, hidden_channels)\n","        self.conv2 = GATConv(hidden_channels, num_classes)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)\n","\n","# Load the TUDataset\n","path = 'path/to/your/data'\n","#dataset = TUDataset(path, name='Your_Dataset_Name')\n","dataset = dataset_proteins\n","# Split the dataset into train, val, and test sets\n","# Shuffle the dataset\n","train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n","        dataset, frac_list=[0.6, 0.2, 0.2], shuffle=True)\n","# Create DataLoaders for each dataset\n","train_loader = GraphDataLoader(train_sampler, batch_size=32, shuffle=True)\n","val_loader = GraphDataLoader(val_sampler, batch_size=32, shuffle=False)\n","test_loader = GraphDataLoader(test_sampler, batch_size=32, shuffle=False)\n","\n","# Instantiate the GAT model, loss function, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","graph, _ = dataset[0]\n","num_node_features = graph.ndata['node_attr'].shape[1]\n","model = GAT(num_node_features=num_node_features, hidden_channels=64, num_classes=2).to(device)\n","loss_fn = torch.nn.NLLLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","\n","# Training function\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = loss_fn(out, data.y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","# Evaluation function\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    for data in loader:\n","        data = data.to(device)\n","        with torch.no_grad():\n","            out = model(data)\n","        pred = out.argmax(dim=1)\n","        correct += (pred == data.y).sum().item()\n","    return correct / len(loader.dataset)\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train()\n","    val_acc = evaluate(val_loader)\n","    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}')\n","\n","# Test the model\n","test_acc = evaluate(test_loader)\n","print(f'Test Acc: {test_acc:.4f}')\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import dgl.data\n","import dgl\n","from dgl.nn import GATConv\n","from dgl.dataloading import GraphDataLoader\n","from torch_geometric.nn import global_add_pool\n","\n","from torch_geometric.nn import global_add_pool\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, num_node_features, hidden_channels, num_classes):\n","        super(GAT, self).__init__()\n","        self.conv1 = GATConv(num_node_features, hidden_channels, num_heads=1)\n","        self.conv2 = GATConv(hidden_channels, num_classes, num_heads=1)\n","\n","    def forward(self, g, x):\n","      x = self.conv1(g, x).squeeze(-1)\n","      x = F.relu(x)\n","      x = F.dropout(x, p=0.5, training=self.training)\n","      x = self.conv2(g, x).squeeze(-1)\n","      x = x.view(x.shape[0], -1, x.shape[-1])  # Reshape the tensor to (batch_size, num_nodes, num_classes)\n","      x = x.sum(dim=1)  # Sum the node features for each graph in the batch\n","\n","      # Divide the summed node features by the number of nodes in each graph\n","      batch_num_nodes = g.batch_num_nodes().float().to(x.device)\n","      x = x / batch_num_nodes.view(-1, 1)\n","\n","      return F.log_softmax(x, dim=1)\n","\n","\n","\n","# Load the TUDataset\n","# dataset = dgl.data.TUDataset(name='Your_Dataset_Name')\n","dataset = dataset_proteins\n","\n","# Split the dataset into train, val, and test sets\n","train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n","        dataset, frac_list=[0.6, 0.2, 0.2], shuffle=True)\n","\n","# Create DataLoaders for each dataset\n","train_loader = GraphDataLoader(train_sampler, batch_size=32, shuffle=True)\n","val_loader = GraphDataLoader(val_sampler, batch_size=32, shuffle=False)\n","test_loader = GraphDataLoader(test_sampler, batch_size=32, shuffle=False)\n","\n","# Instantiate the GAT model, loss function, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","graph, _ = dataset[0]\n","num_node_features = graph.ndata['node_attr'].shape[1]\n","model = GAT(num_node_features=num_node_features, hidden_channels=64, num_classes=2).to(device)\n","loss_fn = torch.nn.NLLLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","\n","# Training function\n","# Training function\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for batched_graph, labels in train_loader:\n","        batched_graph = batched_graph.to(device)\n","        labels = labels.to(device).view(-1) # Reshape the labels\n","        x = batched_graph.ndata['node_attr'].float()\n","        optimizer.zero_grad()\n","        out = model(batched_graph, x)\n","        loss = loss_fn(out, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","\n","# Evaluation function\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    for batched_graph, labels in loader:\n","        batched_graph = batched_graph.to(device)\n","        labels = labels.to(device).view(-1) # Reshape the labels\n","        x = batched_graph.ndata['node_attr'].float()\n","        with torch.no_grad():\n","            out = model(batched_graph, x)\n","        pred = out.argmax(dim=1)\n","        correct += (pred == labels).sum().item()\n","    return correct / len(loader.dataset)\n","\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train()\n","    val_acc = evaluate(val_loader)\n","    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}')\n","\n","# Test the model\n","test_acc = evaluate(test_loader)\n","print(f'Test Acc: {test_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"mzOMiZr_S-7K","executionInfo":{"status":"error","timestamp":1681320315876,"user_tz":-120,"elapsed":317,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"}},"outputId":"bf73f824-abe9-4c06-c03a-9bf7a3be2a98"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-7b35fce5fb61>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-7b35fce5fb61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'node_attr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-7b35fce5fb61>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m# Divide the summed node features by the number of nodes in each graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mbatch_num_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_num_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_num_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2128) must match the size of tensor b (32) at non-singleton dimension 0"]}]},{"cell_type":"markdown","source":["## GAE Implementation"],"metadata":{"id":"Np9ggPBqoy-p"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import dgl\n","from dgl.nn import GraphConv\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from dgl.data import TUDataset\n","\n","# Define GCN layer\n","class GCNLayer(nn.Module):\n","    def __init__(self, in_feats, out_feats):\n","        super(GCNLayer, self).__init__()\n","        self.gcn = GraphConv(in_feats, out_feats)\n","\n","    def forward(self, g, h):\n","        return self.gcn(g, h)\n","\n","# Build the GAE encoder model\n","class Encoder(nn.Module):\n","    def __init__(self, in_feats, hidden_feats, out_feats):\n","        super(Encoder, self).__init__()\n","        self.gcn1 = GCNLayer(in_feats, hidden_feats)\n","        self.gcn2 = GCNLayer(hidden_feats, out_feats)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, g, h):\n","        h = self.gcn1(g, h)\n","        h = self.relu(h)\n","        h = self.gcn2(g, h)\n","        return h\n","\n","# Build the GAE decoder model\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","\n","    def forward(self, z):\n","        z_transpose = z.t()\n","        adj_pred = torch.sigmoid(torch.matmul(z, z_transpose))\n","        return adj_pred\n","\n","# Define the training and evaluation functions\n","# Define the training and evaluation functions\n","def train(encoder, decoder, g, features, adj_orig, optimizer, criterion):\n","    encoder.train()\n","    decoder.train()\n","    optimizer.zero_grad()\n","\n","    z = encoder(g, features)\n","    adj_pred = decoder(z)\n","\n","    loss = criterion(adj_pred, adj_orig)\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()\n","\n","def evaluate(encoder, decoder, g, features, adj_orig, criterion):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    with torch.no_grad():\n","        z = encoder(g, features)\n","        adj_pred = decoder(z)\n","        loss = criterion(adj_pred, adj_orig)\n","\n","    return loss.item()\n","\n","# Prepare the dataset and device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","dataset_proteins = TUDataset(name='PROTEINS')\n","dataset_proteins.graph_lists = [dgl.add_self_loop(graph) for graph in dataset_proteins.graph_lists]\n","\n","# Set parameters\n","graph, _ = dataset_proteins[0]\n","num_node_features = graph.ndata['node_attr'].shape[1]\n","in_feats = num_node_features\n","hidden_feats = 64\n","out_feats = 32\n","lr = 0.01\n","epochs = 100\n","\n","# Prepare the dataset\n","train_dataset, test_dataset = train_test_split(dataset_proteins.graph_lists, test_size=0.1, random_state=42)\n","train_graphs = [graph.to(device) for graph in train_dataset]\n","test_graphs = [graph.to(device) for graph in test_dataset]\n","\n","# Initialize the models and optimizer\n","encoder = Encoder(in_feats, hidden_feats, out_feats).to(device)\n","decoder = Decoder().to(device)\n","model = nn.Sequential(encoder, decoder).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.BCELoss()\n","\n","# Train and evaluate the GAE\n","for epoch in range(epochs):\n","    train_losses = []\n","    for g in train_graphs:\n","        features = g.ndata['node_attr'].float().to(device)\n","        adj_orig = torch.Tensor(g.adjacency_matrix().to_dense()).to(device)\n","        train_loss = train(encoder, decoder, g, features, adj_orig, optimizer, criterion)\n","        train_losses.append(train_loss)\n","\n","    train_loss_mean = np.mean(train_losses)\n","\n","    test_losses = []\n","    for g in test_graphs:\n","        features = g.ndata['node_attr'].float().to(device)\n","        adj_orig = torch.Tensor(g.adjacency_matrix().to_dense()).to(device)\n","        test_loss = evaluate(encoder, decoder, g, features, adj_orig, criterion)\n","        test_losses.append(test_loss)\n","\n","    test_loss_mean = np.mean(test_losses)\n","\n","    print(f'Epoch: {epoch + 1}, Train Loss: {train_loss_mean:.4f}, Test Loss: {test_loss_mean:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hg4eg63rYUSC","executionInfo":{"status":"ok","timestamp":1681326376432,"user_tz":-120,"elapsed":345356,"user":{"displayName":"Deepesh DWIVEDI","userId":"05790688063256084653"}},"outputId":"cd75f52f-3579-40c4-ec2a-cc3655cb6c97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Train Loss: 0.7906, Test Loss: 0.7596\n","Epoch: 2, Train Loss: 0.7302, Test Loss: 0.7337\n","Epoch: 3, Train Loss: 0.7474, Test Loss: 0.7289\n","Epoch: 4, Train Loss: 0.7353, Test Loss: 0.7437\n","Epoch: 5, Train Loss: 0.7246, Test Loss: 0.7656\n","Epoch: 6, Train Loss: 0.7494, Test Loss: 0.7335\n","Epoch: 7, Train Loss: 0.7349, Test Loss: 0.7270\n","Epoch: 8, Train Loss: 0.7084, Test Loss: 0.7264\n","Epoch: 9, Train Loss: 0.7103, Test Loss: 0.7273\n","Epoch: 10, Train Loss: 0.7105, Test Loss: 0.7288\n","Epoch: 11, Train Loss: 0.7085, Test Loss: 0.7375\n","Epoch: 12, Train Loss: 0.7048, Test Loss: 0.7695\n","Epoch: 13, Train Loss: 0.7046, Test Loss: 0.8531\n","Epoch: 14, Train Loss: 0.7150, Test Loss: 0.9180\n","Epoch: 15, Train Loss: 0.7085, Test Loss: 0.9153\n","Epoch: 16, Train Loss: 0.7092, Test Loss: 0.8933\n","Epoch: 17, Train Loss: 0.7085, Test Loss: 0.8942\n","Epoch: 18, Train Loss: 0.7085, Test Loss: 0.8869\n","Epoch: 19, Train Loss: 0.7081, Test Loss: 0.9018\n","Epoch: 20, Train Loss: 0.7085, Test Loss: 0.8500\n","Epoch: 21, Train Loss: 0.7066, Test Loss: 0.9813\n","Epoch: 22, Train Loss: 0.7127, Test Loss: 0.7551\n","Epoch: 23, Train Loss: 0.7066, Test Loss: 1.1067\n","Epoch: 24, Train Loss: 0.7194, Test Loss: 0.7521\n","Epoch: 25, Train Loss: 0.7046, Test Loss: 1.0767\n","Epoch: 26, Train Loss: 0.7176, Test Loss: 0.7317\n","Epoch: 27, Train Loss: 0.7037, Test Loss: 1.0825\n","Epoch: 28, Train Loss: 0.7167, Test Loss: 0.7218\n","Epoch: 29, Train Loss: 0.7043, Test Loss: 0.8748\n","Epoch: 30, Train Loss: 0.7075, Test Loss: 1.1680\n","Epoch: 31, Train Loss: 0.7325, Test Loss: 0.7201\n","Epoch: 32, Train Loss: 0.7553, Test Loss: 0.7210\n","Epoch: 33, Train Loss: 0.7081, Test Loss: 0.7513\n","Epoch: 34, Train Loss: 0.7036, Test Loss: 0.9100\n","Epoch: 35, Train Loss: 0.7120, Test Loss: 0.7664\n","Epoch: 36, Train Loss: 0.7052, Test Loss: 1.0167\n","Epoch: 37, Train Loss: 0.7166, Test Loss: 0.7241\n","Epoch: 38, Train Loss: 0.7034, Test Loss: 1.0872\n","Epoch: 39, Train Loss: 0.7195, Test Loss: 0.7332\n","Epoch: 40, Train Loss: 0.7036, Test Loss: 1.0445\n","Epoch: 41, Train Loss: 0.7149, Test Loss: 0.7231\n","Epoch: 42, Train Loss: 0.7040, Test Loss: 0.8743\n","Epoch: 43, Train Loss: 0.7072, Test Loss: 1.0573\n","Epoch: 44, Train Loss: 0.7198, Test Loss: 0.7201\n","Epoch: 45, Train Loss: 0.7044, Test Loss: 0.7219\n","Epoch: 46, Train Loss: 0.7424, Test Loss: 0.7239\n","Epoch: 47, Train Loss: 0.7053, Test Loss: 0.7206\n","Epoch: 48, Train Loss: 0.7062, Test Loss: 0.7362\n","Epoch: 49, Train Loss: 0.7065, Test Loss: 0.9710\n","Epoch: 50, Train Loss: 0.7141, Test Loss: 0.7210\n","Epoch: 51, Train Loss: 0.7037, Test Loss: 0.9356\n","Epoch: 52, Train Loss: 0.7086, Test Loss: 1.1238\n","Epoch: 53, Train Loss: 0.7269, Test Loss: 0.7201\n","Epoch: 54, Train Loss: 0.7390, Test Loss: 0.7205\n","Epoch: 55, Train Loss: 0.7065, Test Loss: 0.8210\n","Epoch: 56, Train Loss: 0.7113, Test Loss: 0.8256\n","Epoch: 57, Train Loss: 0.7060, Test Loss: 1.0631\n","Epoch: 58, Train Loss: 0.7186, Test Loss: 0.7206\n","Epoch: 59, Train Loss: 0.7141, Test Loss: 0.7200\n","Epoch: 60, Train Loss: 0.7277, Test Loss: 0.7226\n","Epoch: 61, Train Loss: 0.7110, Test Loss: 0.7273\n","Epoch: 62, Train Loss: 0.7044, Test Loss: 0.9256\n","Epoch: 63, Train Loss: 0.7093, Test Loss: 0.8924\n","Epoch: 64, Train Loss: 0.7145, Test Loss: 0.9581\n","Epoch: 65, Train Loss: 0.7105, Test Loss: 0.9026\n","Epoch: 66, Train Loss: 0.7086, Test Loss: 0.9410\n","Epoch: 67, Train Loss: 0.7454, Test Loss: 0.8251\n","Epoch: 68, Train Loss: 0.7295, Test Loss: 0.8576\n","Epoch: 69, Train Loss: 0.7149, Test Loss: 0.8238\n","Epoch: 70, Train Loss: 0.7058, Test Loss: 0.8785\n","Epoch: 71, Train Loss: 0.7071, Test Loss: 0.8462\n","Epoch: 72, Train Loss: 0.7061, Test Loss: 0.8716\n","Epoch: 73, Train Loss: 0.7085, Test Loss: 0.7298\n","Epoch: 74, Train Loss: 0.7076, Test Loss: 0.8562\n","Epoch: 75, Train Loss: 0.7070, Test Loss: 1.1096\n","Epoch: 76, Train Loss: 0.7172, Test Loss: 0.7209\n","Epoch: 77, Train Loss: 0.7053, Test Loss: 0.7426\n","Epoch: 78, Train Loss: 0.7037, Test Loss: 1.0472\n","Epoch: 79, Train Loss: 0.7215, Test Loss: 0.7201\n","Epoch: 80, Train Loss: 0.7102, Test Loss: 0.7217\n","Epoch: 81, Train Loss: 0.7193, Test Loss: 0.7206\n","Epoch: 82, Train Loss: 0.7027, Test Loss: 0.9113\n","Epoch: 83, Train Loss: 0.7170, Test Loss: 0.7260\n","Epoch: 84, Train Loss: 0.7446, Test Loss: 0.7202\n","Epoch: 85, Train Loss: 0.7091, Test Loss: 0.7209\n","Epoch: 86, Train Loss: 0.8127, Test Loss: 0.7249\n","Epoch: 87, Train Loss: 0.7062, Test Loss: 0.7203\n","Epoch: 88, Train Loss: 0.7053, Test Loss: 0.7724\n","Epoch: 89, Train Loss: 0.7331, Test Loss: 1.0900\n","Epoch: 90, Train Loss: 0.7205, Test Loss: 0.7466\n","Epoch: 91, Train Loss: 0.7045, Test Loss: 1.0986\n","Epoch: 92, Train Loss: 0.7231, Test Loss: 0.7229\n","Epoch: 93, Train Loss: 0.7035, Test Loss: 1.0830\n","Epoch: 94, Train Loss: 0.7256, Test Loss: 0.7245\n","Epoch: 95, Train Loss: 0.7136, Test Loss: 0.7200\n","Epoch: 96, Train Loss: 0.7417, Test Loss: 0.7240\n","Epoch: 97, Train Loss: 0.7033, Test Loss: 0.7188\n","Epoch: 98, Train Loss: 0.7090, Test Loss: 0.7437\n","Epoch: 99, Train Loss: 0.7048, Test Loss: 0.9610\n","Epoch: 100, Train Loss: 0.7120, Test Loss: 0.7228\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ciJms8Cfo9Jy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1oMb-gW6NfPKelPPng2kdgD3Sz5e0ApYR","timestamp":1682164444476}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}